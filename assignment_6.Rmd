---
title: "Machine learning - Assignment 6"
author: "Alice Tivarovsky"
date: "2/27/2020"
output: 
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

## Setup 

Loading the necessary packages for data cleaning, modeling, partitioning, nhanes interface, and classification. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(100)

library(tidyverse)
library(caret)
library(glmnet)
library(e1071)
library(RNHANES)
library(rpart)
library(rpart.plot)
library(pROC)
```

Comparison between Classification Trees, SVM and Logistic Regression

The posted article by Yu et al utilized NHANES data from 1999-2004 to predict diabetes and pre-diabetes using Support Vector Machines. You will conduct a similar analysis using data within the NHANES package in R. For this exercise, you will try to predict Diabetes using similar (although not all) variables. The available data is also slightly different, so you likely won't get the same answers.


## Number 1: Loading NHANES, Restricting and Partitioning

Restrict the NHANES data to the list of 12 variables below. Partition the data into training and testing using a 70/30 split.

"Age", "Gender", "Race1", "Education", "HHIncome" (DEMO), "Weight", "Height" (BMXBMI), "Pulse" (BPX), "Diabetes" (DIQ), "BMI" (BMXBMI), "PhysActive", "Smoke100" (SMQ)

```{r load and tidy}

# Loading
demo_99 = nhanes_load_data("DEMO", "1999-2000") 
demo_01 = nhanes_load_data("DEMO", "2001-2002")
demo_03 = nhanes_load_data("DEMO", "2003-2004")                           

bmx_99 = nhanes_load_data("BMX", "1999-2000")
bmx_01 = nhanes_load_data("BMX", "2001-2002")
bmx_03 = nhanes_load_data("BMX", "2003-2004")  

bpx_99 = nhanes_load_data("BPX", "1999-2000")
bpx_01 = nhanes_load_data("BPX", "2001-2002")
bpx_03 = nhanes_load_data("BPX", "2003-2004")  

paq_99 = nhanes_load_data("PAQ", "1999-2000")
paq_01 = nhanes_load_data("PAQ", "2001-2002")
paq_03 = nhanes_load_data("PAQ", "2003-2004")  

diq_99 = nhanes_load_data("DIQ", "1999-2000")
diq_01 = nhanes_load_data("DIQ", "2001-2002")
diq_03 = nhanes_load_data("DIQ", "2003-2004")  

smq_99 = nhanes_load_data("SMQ", "1999-2000")
smq_01 = nhanes_load_data("SMQ", "2001-2002")
smq_03 = nhanes_load_data("SMQ", "2003-2004")  
                           
# Joining
data_99 = 
  left_join(demo_99, bmx_99, by = "SEQN") %>% 
  left_join(bpx_99, by = "SEQN") %>% 
  left_join(paq_99, by = "SEQN") %>% 
  left_join(diq_99, by = "SEQN") %>% 
  left_join(smq_99, by = "SEQN") %>% 
  select("SEQN", "RIDAGEYR", "RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDHHINC", "BMXBMI", "BMXHT", "BMXWT", "BPXPLS", "DIQ010", "PAQ180", "SMQ020") 

data_01 = 
  left_join(demo_01, bmx_01, by = "SEQN") %>% 
  left_join(bpx_01, by = "SEQN") %>% 
  left_join(paq_01, by = "SEQN") %>% 
  left_join(diq_01, by = "SEQN") %>% 
  left_join(smq_01, by = "SEQN") %>% 
  select("SEQN", "RIDAGEYR", "RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDHHINC", "BMXBMI", "BMXHT", "BMXWT", "BPXPLS", "DIQ010", "PAQ180", "SMQ020") 

data_03 = 
  left_join(demo_03, bmx_03, by = "SEQN") %>% 
  left_join(bpx_03, by = "SEQN") %>% 
  left_join(paq_03, by = "SEQN") %>% 
  left_join(diq_03, by = "SEQN") %>% 
  left_join(smq_03, by = "SEQN") %>% 
  select("SEQN", "RIDAGEYR", "RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDHHINC", "BMXBMI", "BMXHT", "BMXWT", "BPXPLS", "DIQ010", "PAQ180", "SMQ020") 

final_data = bind_rows(data_99, data_01, data_03) %>% 
  janitor::clean_names() %>% 
  filter(diq010 %in% c(1,2)) %>% 
  mutate(diq010 = as.factor(diq010), 
         riagendr = as.factor(riagendr), 
         ridreth1 = as.factor(ridreth1), 
         dmdeduc2 = as.factor(dmdeduc2), 
         indhhinc = as.factor(indhhinc), 
         paq180 = as.factor(paq180), 
         smq020 = as.factor(smq020)) %>% 
  drop_na()
```

Partitioning: 

```{r partition}

training_data = final_data$diq010 %>% createDataPartition(p = 0.7, list = F)
train_data = final_data[training_data, ]
test_data = final_data[-training_data, ]

```


## Number 2: Prediction Models

Construct three prediction models to predict diabetes using the 11 features from NHANES. You will use the following three algorithms to create your prediction models:

a) Classification Tree

b) Support Vector Classifier (i.e. Support Vector Machine with a linear classifier)

c) Logistic regression.

### Classification Tree

```{r classification tree}

train_control = trainControl(method = "cv", number = 10)
grid_2 = expand.grid(cp = seq(0.0001, 0.011, by = 0.0001))
tree_diabetes = train(diq010 ~., data = train_data,  method = "rpart", trControl = train_control, tuneGrid = grid_2)
tree_diabetes$bestTune

tree_diabetes
varImp(tree_diabetes)
rpart.plot(tree_diabetes$finalModel)

```

The optimal Cp was 0.0047. 

### Support Vector Classifier

```{r svm}

svm_diabetes = svm(diq010 ~ ., data = train_data, kernel="linear", cost = 0.1, scale = TRUE)
print(svm_diabetes)

new_data = train_data[-11]

svm_pred = predict(svm_diabetes, new_data)
table(svm_pred, train_data$diq010)

misClasificError = mean(svm_pred != train_data$diq010, na.rm = T)
print(paste('Accuracy Model 1',1 - misClasificError))

features = new_data
outcome = train_data$diq010

svm_tune = tune(svm, diq010 ~ ., data = train_data,  kernel="linear", range=list(cost=10^(-1:1)))

summary(svm_tune)

```

Accuracy of the SVM model was also 0.90. 

### Logistic Regression 

```{r logistic}

log_model = glm(diq010 ~ ., data = train_data, family = binomial(link = "logit")) 
log_model

# Make predictions
log_model %>% predict(test_data)

```


## Number 3 

In the classification tree, the optimal hyperparameter Cp for highest was 0.0047. 

```{r}

train_control = trainControl(method = "cv", number = 10)
grid_2 = expand.grid(cp = seq(0.0001, 0.011, by = 0.0001))
tree_diabetes = train(diq010 ~., data = train_data,  method = "rpart", trControl = train_control, tuneGrid = grid_2)
tree_diabetes$bestTune

tree_diabetes
varImp(tree_diabetes)
rpart.plot(tree_diabetes$finalModel)

```

The accuracy of the classification tree at a complexity parameter of 0.0047 was 90.26%. The accuracy of the support vector machine was 90.26%. 

Log model accuracy: 

```{r}

pred_log = predict(log_model, train_data)
pred_log_class = ifelse(pred_log > 0.5,1,0)
misClasificError = mean(pred_log_class != train_data$diq010, na.rm = T)
print(paste('Accuracy Log Model',1 - misClasificError))

```

The accuracy of the logistic model is only 8.35%. 

## Number 4

The classification tree appears to have been the most appropriate here. The support vector machine did not work as expected. We will now calculate accuracy in the final test set for the classification tree. 

```{r}

pred_diabetes<-predict(tree_diabetes, test_data)
pred_diabetes_prob<-predict(tree_diabetes, test_data, type="prob")

eval.results<-confusionMatrix(pred_diabetes, test_data$diq010, positive = "1")
print(eval.results)

```

The accuracy for the classification tree is 90.26%. 

## Number 5

One major limitation of classification trees is model instability. Updates to the data, which occur with every annual NHANES cycle, are likely to completely change the structure of the tree, thereby limiting its interpretability over the long term. 

Another limitation of classification trees is that the algorithm is "greedy" - meaning it only looks at the current node, not downstream nodes to make a splitting decision.  

